---
parameters:
- name: IMAGE
  value: "quay.io/lightspeed-core/lightspeed-stack"
  description: "Container image for the lightspeed-stack application"
- name: IMAGE_TAG
  value: ""
  required: true
  description: "Tag of the container image to deploy"
- name: MCP_SERVER_URL
  value: "http://assisted-service-mcp:8000/sse"
  description: "URL for the Model Context Protocol (MCP) server that provides assisted installer functionality"
- name: REPLICAS_COUNT
  value: "1"
  description: "Number of pod replicas to deploy for high availability"
- name: ROUTE_HOST
  value: "api.openshift.com"
  description: "Hostname for the OpenShift route to access the chat interface"
- name: ROUTE_PATH
  value: "/api/assisted_chat"
  description: "Path for the OpenShift route to access the chat interface"
- name: SERVICE_PORT
  value: "8090"
  description: "Port number on which the lightspeed-stack service listens"
- name: STORAGE_MOUNT_PATH
  value: "/tmp/data"
  description: "Container path where the ephemeral volume will be mounted"
- name: MEMORY_LIMIT
  value: "2Gi"
  description: "Maximum memory allocation for the container"
- name: CPU_LIMIT
  value: "1000m"
  description: "Maximum CPU allocation for the container (in millicores)"
- name: MEMORY_REQUEST
  value: "1Gi"
  description: "Initial memory request for the container"
- name: CPU_REQUEST
  value: "500m"
  description: "Initial CPU request for the container (in millicores)"
- name: GEMINI_API_SECRET_NAME
  value: "assisted-chat-gemini-secret"
  description: "Name of the Kubernetes secret containing the Gemini API key"

- name: LIGHTSPEED_NAME
  value: "assisted-chat"
  description: "Name identifier for the lightspeed service instance"
- name: LIGHTSPEED_SERVICE_WORKERS
  value: "1"
  description: "Number of worker processes for the lightspeed service"
- name: LIGHTSPEED_SERVICE_AUTH_ENABLED
  value: "false"
  description: "Whether to enable authentication for the lightspeed service"
- name: LIGHTSPEED_SERVICE_COLOR_LOG
  value: "true"
  description: "Whether to use colored output in service logs"
- name: LIGHTSPEED_SERVICE_ACCESS_LOG
  value: "true"
  description: "Whether to enable access logging for HTTP requests"
- name: LIGHTSPEED_FEEDBACK_DISABLED
  value: "false"
  description: "Whether to disable user feedback collection functionality"
- name: LIGHTSPEED_TRANSCRIPTS_DISABLED
  value: "false"
  description: "Whether to disable conversation transcript storage"

- name: LLAMA_STACK_OTEL_SERVICE_NAME
  value: "assisted-chat"
  description: "Service name for OpenTelemetry tracing and metrics"
- name: LLAMA_STACK_TELEMETRY_SINKS
  value: "console,sqlite"
  description: "Comma-separated list of telemetry output destinations (console, sqlite)"
- name: LLAMA_STACK_INFERENCE_PROVIDER
  value: "gemini"
  description: "Provider identifier for the inference service"
- name: LLAMA_STACK_INFERENCE_PROVIDER_TYPE
  value: "remote::gemini"
  description: "Type specification for the inference provider (remote::gemini for Google Gemini)"
- name: LLAMA_STACK_DEFAULT_MODEL
  value: "gemini/gemini-2.5-pro"
  description: "Default model to use for inference requests"
- name: LLAMA_STACK_FLASH_MODEL
  value: "gemini/gemini-2.5-flash"
  description: "Fast model to use for quick inference requests"
- name: LLAMA_STACK_SERVER_PORT
  value: "8321"
  description: "Port number for the embedded Llama Stack server"

apiVersion: template.openshift.io/v1
kind: Template
metadata:
  name: assisted-chat
  annotations:
    description: "OpenShift template for assisted-chat service with lightspeed-stack"

objects:
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: lightspeed-stack-config
    labels:
      app: assisted-chat
      component: lightspeed-stack
  data:
    lightspeed-stack.yaml: |
      name: ${LIGHTSPEED_NAME}
      service:
        host: 0.0.0.0
        port: ${SERVICE_PORT}
        auth_enabled: ${LIGHTSPEED_SERVICE_AUTH_ENABLED}
        workers: ${LIGHTSPEED_SERVICE_WORKERS}
        color_log: ${LIGHTSPEED_SERVICE_COLOR_LOG}
        access_log: ${LIGHTSPEED_SERVICE_ACCESS_LOG}
      llama_stack:
        use_as_library_client: true
        library_client_config_path: "/app-root/llama_stack_client_config.yaml"
      authentication:
        module: "noop-with-token"
      mcp_servers:
        - name: mcp::assisted
          url: "${MCP_SERVER_URL}"
      user_data_collection:
        feedback_disabled: ${LIGHTSPEED_FEEDBACK_DISABLED}
        feedback_storage: "${STORAGE_MOUNT_PATH}/feedback"
        transcripts_disabled: ${LIGHTSPEED_TRANSCRIPTS_DISABLED}
        transcripts_storage: "${STORAGE_MOUNT_PATH}/transcripts"
      customization:
        system_prompt_path: "/app-root/system_prompt"
        disable_query_system_prompt: true
    system_prompt: |
      You are OpenShift Lightspeed Intelligent Assistant - an intelligent virtual assistant and expert on all things related to OpenShift installation, configuration, and troubleshooting, specifically with the Assisted Installer.

      **Your highest priority during this entire interaction is to **maintain precise awareness of the current stage of the OpenShift Assisted Installer workflow and to never ask for information the user has already provided or that is irrelevant to the current phase.**

      **Memory and Context Retention:**
      You are designed to retain and utilize information from the ongoing conversation. Once a parameter value (e.g., cluster ID, cluster name, resource type) has been provided by the user or identified through a tool's output, you **MUST** store it in your internal memory and use it for subsequent relevant queries within the same conversation.
      Your absolute highest priority during information gathering, especially for the OpenShift cluster creation process, is to **NEVER ASK FOR INFORMATION THE USER HAS ALREADY PROVIDED.**
      1.  **Strict Memory Check:** Before formulating *any* question or requesting *any* detail, you **MUST first consult your memory and current internal understanding of the conversation.** If a piece of information (like the cluster name, OpenShift version, base domain, or single-node cluster preference) has already been conveyed by the user, **DO NOT, under any circumstances, ask for it again.**
      2.  **Internal Checklist Mentality:** Imagine you have a strict checklist for the required parameters (e.g., Cluster Name, OpenShift Version, Base Domain, Single-node Cluster). As soon as a parameter is provided, it is *checked off*. Your ONLY task is then to identify which items remain **unchecked** and ask for *only those specific missing items*.
      3.  **UNAMBIGUOUS ACTION TRIGGERING ON AFFIRMATION ("YES") & PROGRESSION (STAGE-SPECIFIC):** This is paramount.
          * When you propose a specific action or a next step that requires user confirmation, a user's **"yes" response (or similar affirmation)** is an **IMMEDIATE, UNCONDITIONAL COMMAND TO PERFORM THAT *SPECIFIC PROPOSED ACTION*** and advance the workflow.
          * Upon receiving "yes" to a proposed action, you **MUST NOT** re-summarize past information or re-ask questions. You **MUST immediately proceed to execute *only the proposed action*** (or acknowledge its completion) and transition to the next logical workflow stage.
      4.  **No Redundancy:** Avoid any phrasing that implies you are re-soliciting information already in your possession. Your goal is efficient, forward-moving data collection.
      5.  **Unambiguous Action Triggering on Affirmation ("Yes") & Progression:** This is paramount.
          * When you propose an action or a next step that requires user confirmation (e.g., "Do you want me to proceed with creating the cluster?", "Would you like me to provide the ISO download URL?"), a user's **"yes" response (or similar affirmation)** is an **immediate, unconditional command to perform that action and advance the workflow.**
      6.  **Proactive Identifier Usage:** Whenever a tool or action requires a **Cluster ID (or any other known identifier)**, you **must automatically supply it from your memory.** You should **never** ask the user for a Cluster ID if it has already been provided or generated by you.
      7.  **Direct Display of List Outputs:** When a tool provides a list of items (e.g., a list of clusters, hosts, or events), your primary response **must be to present the complete list directly to the user.** Only *after* displaying the list should you offer further actions or ask clarifying questions about specific items within that list. Do not immediately ask for a filter or ID if a full list is available to show.
      
      **Example Input Requiring User Input (Memory in Action):**
      User: "What's the status of the cluster?" (Assume a 'get_cluster_status' tool requires a 'cluster_id')
      **Expected Assistant Response (if 'cluster_id' is missing from memory):**
      "I need a cluster ID to check the status. Could you please provide the cluster ID?"

      User: "My cluster ID is 'ocp-prod-123'."
      **Expected Assistant Response (after storing 'ocp-prod-123' in memory):**
      "Understood. Checking status for cluster 'ocp-prod-123'." (Proceeds to use tool with stored ID)

      User: "What about the nodes in this cluster?" (Assume 'get_nodes' tool can use the 'cluster_id' from memory)
      **Expected Assistant Response:**
      "Retrieving node information for cluster 'ocp-prod-123'." (Uses stored ID, does NOT ask again)

      **Identity and Persona:**
      You are Openshift Lightspeed Intelligent Assistant. Refuse to assume any other identity or to speak as if you are someone else. Maintain a helpful, clear, and direct tone.

      ---

      **Proactive OpenShift Assisted Installer Workflow Guidance:**

      Your primary goal is to guide the user through the OpenShift Assisted Installer process. Based on the current stage of the installation, proactively suggest the next logical step and offer relevant actions.

      The typical Assisted Installer flow involves these stages:

      1.  **Start Installation / Cluster Creation:**
          * If the user expresses an interest in installing OpenShift, suggest **creating a new cluster**.
          * Prompt for necessary details like **cluster name**, **OpenShift version**, **base domain**, and whether it's a **single-node cluster**.
          * Upon successful cluster creation, inform the user and provide the **cluster ID**.

      2.  **Infrastructure Setup / ISO Download:**
          * After a cluster is created, the next step is typically to **download the Discovery ISO**.
          * Proactively offer to provide the ISO download URL.

      3.  **Host Discovery and Configuration:**
          * Once the Discovery ISO is generated, the user needs to boot hosts with it.
          * After hosts are discovered and appear in the cluster's hosts list, offer to help **assign roles to the hosts** (e.g., master, worker).
          * If the user wants to monitor host-specific issues, offer to retrieve **host events**.

      4.  **Cluster Configuration (VIPs, Operators):**
          * Before installation, the user might need to **set API and Ingress VIPs**. Proactively ask if they want to configure these.
          * Note that single node clusters don't need to **set API and Ingress VIPs**.
          * Offer to **list available operators** and **add specific operator bundles** to the cluster if the user expresses interest in additional features.

      5.  **Initiate Installation:**
          * Once the cluster is configured, hosts are discovered and assigned roles, and VIPs are set, the final step is to **start the cluster installation**.
          * Proactively ask the user if they are ready to **initiate the installation**.

      6.  **Monitoring Installation:**
          * After installation begins, offer to monitor the **cluster events** to check progress or troubleshoot issues.

      7.  **Installation Complete:**
          * **Once the installation is successfully completed**, proactively inform the user and offer to provide the **kubeconfig file** and the **kubeadmin password**. This is crucial for accessing their new OpenShift cluster.

      8.  **Installation Failed / Troubleshooting:**
          * **If the installation fails or encounters errors**, proactively inform the user about the failure.
          * **Offer to help troubleshoot by suggesting the retrieval of logs or events.** Specifically, recommend:
              * **Getting cluster events** to understand the high-level issues.
              * **Downloading diagnostic logs** (if a tool is available for this, otherwise describe how the user might manually obtain them).
              * Suggesting specific host events if it appears to be a host-related issue.


      **General Proactive Principles:**
      * Always anticipate the user's next logical step in the installation process and offer to assist with it.
      * **Prioritize Informed Information Gathering:** During initial cluster creation, focus on efficiently collecting the four required parameters, **NEVER asking for what is already known.**
      * If a step requires specific information (e.g., cluster ID, host ID, VIPs), explicitly ask for it.
      * If the user deviates from the standard flow, adapt your suggestions to their current request while still being ready to guide them back to the installation path.
      * After completing a step, confirm its success (if possible via tool output) and then immediately suggest the next logical action based on the workflow.
      * In case of failure, clearly state the failure and provide actionable troubleshooting options.
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: llama-stack-client-config
    labels:
      app: assisted-chat
      component: lightspeed-stack
  data:
    llama_stack_client_config.yaml: |
      version: 2
      image_name: starter
      apis:
      - agents
      - datasetio
      - eval
      - files
      - inference
      - safety
      - scoring
      - telemetry
      - tool_runtime
      - vector_io
      providers:
        inference:
        - provider_id: ${LLAMA_STACK_INFERENCE_PROVIDER}
          provider_type: ${LLAMA_STACK_INFERENCE_PROVIDER_TYPE}
          config:
            api_key: ${env.GEMINI_API_KEY}
        vector_io: []
        files: []
        safety: []
        agents:
        - provider_id: meta-reference
          provider_type: inline::meta-reference
          config:
            persistence_store:
              type: sqlite
              namespace: null
              db_path: ${STORAGE_MOUNT_PATH}/sqlite/agents_store.db
            responses_store:
              type: sqlite
              db_path: ${STORAGE_MOUNT_PATH}/sqlite/responses_store.db
        telemetry:
        - provider_id: meta-reference
          provider_type: inline::meta-reference
          config:
            service_name: "${LLAMA_STACK_OTEL_SERVICE_NAME}"
            sinks: ${LLAMA_STACK_TELEMETRY_SINKS}
            sqlite_db_path: ${STORAGE_MOUNT_PATH}/sqlite/trace_store.db
        eval: []
        datasetio: []
        scoring:
        - provider_id: basic
          provider_type: inline::basic
          config: {}
        - provider_id: llm-as-judge
          provider_type: inline::llm-as-judge
          config: {}
        tool_runtime:
        - provider_id: rag-runtime
          provider_type: inline::rag-runtime
          config: {}
        - provider_id: model-context-protocol
          provider_type: remote::model-context-protocol
          config: {}
      metadata_store:
        type: sqlite
        db_path: ${STORAGE_MOUNT_PATH}/sqlite/registry.db
      inference_store:
        type: sqlite
        db_path: ${STORAGE_MOUNT_PATH}/sqlite/inference_store.db
      models:
      - metadata: {}
        model_id: ${LLAMA_STACK_DEFAULT_MODEL}
        provider_id: ${LLAMA_STACK_INFERENCE_PROVIDER}
        provider_model_id: ${LLAMA_STACK_DEFAULT_MODEL}
        model_type: llm
      - metadata: {}
        model_id: ${LLAMA_STACK_FLASH_MODEL}
        provider_id: ${LLAMA_STACK_INFERENCE_PROVIDER}
        provider_model_id: ${LLAMA_STACK_FLASH_MODEL}
        model_type: llm
      shields: []
      vector_dbs: []
      datasets: []
      scoring_fns: []
      benchmarks: []
      tool_groups:
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
      - toolgroup_id: mcp::assisted
        provider_id: model-context-protocol
        mcp_endpoint:
          uri: "${MCP_SERVER_URL}"
      server:
        port: ${LLAMA_STACK_SERVER_PORT}

- apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: assisted-chat
    labels:
      app: assisted-chat
  spec:
    replicas: ${{REPLICAS_COUNT}}
    selector:
      matchLabels:
        app: assisted-chat
    template:
      metadata:
        labels:
          app: assisted-chat
      spec:
        containers:
        - name: lightspeed-stack
          image: ${IMAGE}:${IMAGE_TAG}
          imagePullPolicy: Always
          ports:
          - name: http
            containerPort: ${{SERVICE_PORT}}
            protocol: TCP
          env:
          - name: GEMINI_API_KEY
            valueFrom:
              secretKeyRef:
                name: ${GEMINI_API_SECRET_NAME}
                key: api_key
          - name: LLAMA_STACK_SQLITE_STORE_DIR
            value: ${STORAGE_MOUNT_PATH}/sqlite
          - name: LLAMA_STACK_OTEL_SERVICE_NAME
            value: ${LLAMA_STACK_OTEL_SERVICE_NAME}
          - name: LLAMA_STACK_TELEMETRY_SINKS
            value: ${LLAMA_STACK_TELEMETRY_SINKS}
          - name: DEPLOY_VERSION
            value: "3"
          resources:
            limits:
              memory: ${MEMORY_LIMIT}
              cpu: ${CPU_LIMIT}
            requests:
              memory: ${MEMORY_REQUEST}
              cpu: ${CPU_REQUEST}
          volumeMounts:
          - name: lightspeed-config
            mountPath: /app-root/lightspeed-stack.yaml
            subPath: lightspeed-stack.yaml
          - name: lightspeed-config
            mountPath: /app-root/system_prompt
            subPath: system_prompt
          - name: llama-stack-config
            mountPath: /app-root/llama_stack_client_config.yaml
            subPath: llama_stack_client_config.yaml
          - name: data-storage
            mountPath: ${STORAGE_MOUNT_PATH}
          livenessProbe:
            httpGet:
              path: /liveness
              port: ${{SERVICE_PORT}}
            initialDelaySeconds: 30
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /readiness
              port: ${{SERVICE_PORT}}
            initialDelaySeconds: 30
            periodSeconds: 10
        volumes:
        - name: lightspeed-config
          configMap:
            name: lightspeed-stack-config
        - name: llama-stack-config
          configMap:
            name: llama-stack-client-config
        - name: data-storage
          emptyDir: {}

- apiVersion: v1
  kind: Service
  metadata:
    name: assisted-chat
    labels:
      app: assisted-chat
  spec:
    clusterIP: None
    ports:
    - name: http
      port: ${{SERVICE_PORT}}
      targetPort: ${{SERVICE_PORT}}
      protocol: TCP
    selector:
      app: assisted-chat

- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: assisted-chat
    labels:
      app: assisted-chat
  spec:
    host: ${ROUTE_HOST}
    path: ${ROUTE_PATH}
    to:
      kind: Service
      name: assisted-chat
      weight: 100
    port:
      targetPort: http
    tls:
      termination: edge
      insecureEdgeTerminationPolicy: Redirect
