# vim: set filetype=dockerfile
FROM localhost/local-ai-chat-lightspeed-stack:latest

USER root

ADD ./llama-stack /app-root/llama-stack

RUN python3.12 -m ensurepip

RUN cd /app-root/llama-stack && python3.12 -m pip install --editable .

RUN cd /app-root/ && python3.12 -m pip install .

# Enable async compatibility in the inference provider
# See https://github.com/meta-llama/llama-stack/pull/3029/files
# Remove this once lightspeed-core bump the llama-stack version
RUN sed -i 's/response = litellm\.completion(\*\*params)/response = await litellm\.acompletion(\*\*params)/' /app-root/llama-stack/llama_stack/providers/utils/inference/litellm_openai_mixin.py
RUN sed -i 's/for chunk in response:/async for chunk in response:/' /app-root/llama-stack/llama_stack/providers/utils/inference/litellm_openai_mixin.py

RUN python3.12 -m pip install pyyaml pyaml

RUN python3.12 -m pip install litellm

RUN python3.12 -m pip install sqlalchemy

RUN python3.12 -m pip install mcp

RUN python3.12 -m pip install psycopg2-binary

EXPOSE 8080

